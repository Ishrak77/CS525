{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd748d1-f7e3-4e73-b2d6-3efc61414070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoding path (Downsampling)\n",
    "        self.encoder1 = self.conv_block(in_channels, 64)\n",
    "        self.encoder2 = self.conv_block(64, 128)\n",
    "        self.encoder3 = self.conv_block(128, 256)\n",
    "        self.encoder4 = self.conv_block(256, 512)\n",
    "        \n",
    "        # Bottleneck (Center of the network)\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "        \n",
    "        # Decoding path (Upsampling)\n",
    "        self.decoder4 = self.deconv_block(1024, 512)\n",
    "        self.decoder3 = self.deconv_block(512, 256)\n",
    "        self.decoder2 = self.deconv_block(256, 128)\n",
    "        self.decoder1 = self.deconv_block(128, 64)\n",
    "        \n",
    "        # Output layer\n",
    "        self.output = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        A convolutional block with two Conv2D layers, followed by ReLU activation and BatchNorm.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def deconv_block(self, in_channels, out_channels):\n",
    "        \"\"\"\n",
    "        A transposed convolutional block to upsample the features.\n",
    "        \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2, padding=0),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoding path\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(F.max_pool2d(enc1, 2))\n",
    "        enc3 = self.encoder3(F.max_pool2d(enc2, 2))\n",
    "        enc4 = self.encoder4(F.max_pool2d(enc3, 2))\n",
    "        \n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))\n",
    "        \n",
    "        # Decoding path\n",
    "        dec4 = self.decoder4(bottleneck)\n",
    "        dec4 = torch.cat([dec4, enc4], dim=1)  # Concatenate skip connection\n",
    "        dec4 = self.conv_block(dec4.size(1), 512)(dec4)  # Apply a conv block on concatenated features\n",
    "        \n",
    "        dec3 = self.decoder3(dec4)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)\n",
    "        dec3 = self.conv_block(dec3.size(1), 256)(dec3)\n",
    "        \n",
    "        dec2 = self.decoder2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)\n",
    "        dec2 = self.conv_block(dec2.size(1), 128)(dec2)\n",
    "        \n",
    "        dec1 = self.decoder1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)\n",
    "        dec1 = self.conv_block(dec1.size(1), 64)(dec1)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.output(dec1)\n",
    "        return out\n",
    "\n",
    "def train_model(dataloader, model, criterion, optimizer, num_epochs=10):\n",
    "    \n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, masks in dataloader:\n",
    "            # Move the data to the same device as the model (GPU/CPU)\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track the loss\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print statistics after each epoch\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}')\n",
    "    \n",
    "    print(\"Training completed.\")\n",
    "\n",
    "class UNetDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, train_files, data_dir, image_dim=(128, 128)):\n",
    "        \"\"\"\n",
    "        Dataset for loading images and masks from HDF5 files.\n",
    "\n",
    "        Parameters:\n",
    "        - train_files: List of file names of the training HDF5 files.\n",
    "        - data_dir: Directory where the HDF5 files are stored.\n",
    "        - image_dim: Desired dimensions of the output images and masks (height, width).\n",
    "        \"\"\"\n",
    "        self.train_files = train_files\n",
    "        self.data_dir = data_dir\n",
    "        self.image_dim = image_dim\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.train_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Load a single sample from the HDF5 file, apply transformations, and return the image and mask.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx: Index of the sample to be loaded.\n",
    "        \n",
    "        Returns:\n",
    "        - image: Tensor of shape (C, H, W) for the image.\n",
    "        - mask: Tensor of shape (1, H, W) for the mask.\n",
    "        \"\"\"\n",
    "        file_path = os.path.join(self.data_dir, self.train_files[idx])\n",
    "        with h5py.File(file_path, 'r') as hf:\n",
    "            # Check for the correct keys\n",
    "            if 'image' in hf.keys() and 'mask' in hf.keys():\n",
    "                image = hf['image'][:]  # Expected shape: (H, W, 4)\n",
    "                mask = hf['mask'][:]    # Expected shape: (H, W) or (H, W, C)\n",
    "            else:\n",
    "                raise KeyError(f\"Unexpected keys in {self.train_files[idx]}: {list(hf.keys())}\")\n",
    "\n",
    "            # Resize images if necessary\n",
    "            if image.shape[:2] != self.image_dim:\n",
    "                image = resize(\n",
    "                    image, \n",
    "                    (*self.image_dim, image.shape[2]), \n",
    "                    preserve_range=True, \n",
    "                    anti_aliasing=True\n",
    "                )\n",
    "\n",
    "            if mask.shape != self.image_dim:\n",
    "                # Use nearest-neighbor interpolation for masks to preserve labels\n",
    "                mask = resize(\n",
    "                    mask, \n",
    "                    self.image_dim, \n",
    "                    preserve_range=True, \n",
    "                    order=0, \n",
    "                    anti_aliasing=False\n",
    "                )\n",
    "\n",
    "            # Normalize the image\n",
    "            image_max = np.max(image)\n",
    "            if image_max > 0:\n",
    "                image = image.astype('float32') / image_max\n",
    "            else:\n",
    "                image = image.astype('float32')\n",
    "\n",
    "            # Normalize the mask\n",
    "            mask_max = np.max(mask)\n",
    "            if mask_max > 0:\n",
    "                mask = mask.astype('float32') / mask_max\n",
    "            else:\n",
    "                mask = mask.astype('float32')  # All zeros\n",
    "\n",
    "            # Convert to tensor and reorder channels to (C, H, W)\n",
    "            image = torch.tensor(image.transpose(2, 0, 1), dtype=torch.float32)  # C, H, W\n",
    "                \n",
    "            mask = torch.tensor(mask.transpose(2, 0, 1), dtype=torch.float32)  # C, H, W\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1b713a8-c7d4-43a4-bdcb-990660170e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = UNet(in_channels=4, out_channels=3)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function (Binary Cross-Entropy Loss for binary segmentation)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer (Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0cf1fca-cbd1-4a3c-a4c7-895e92497615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 62\n",
      "Training samples: 49\n",
      "Validation samples: 13\n",
      "Epoch [1/10], Loss: 0.7138\n",
      "Epoch [2/10], Loss: 0.7152\n",
      "Epoch [3/10], Loss: 0.7128\n",
      "Epoch [4/10], Loss: 0.7120\n",
      "Epoch [5/10], Loss: 0.7122\n",
      "Epoch [6/10], Loss: 0.7138\n",
      "Epoch [7/10], Loss: 0.7105\n",
      "Epoch [8/10], Loss: 0.7096\n",
      "Epoch [9/10], Loss: 0.7093\n",
      "Epoch [10/10], Loss: 0.7098\n",
      "Training completed.\n"
     ]
    }
   ],
   "source": [
    "# Define the data directory (adjust this path if necessary)\n",
    "data_dir = 'BraTS2020_training_data/content/small_data/'\n",
    "\n",
    "# Get list of .h5 files\n",
    "h5_files = [f for f in os.listdir(data_dir) if f.endswith('.h5')]\n",
    "\n",
    "# Ensure that there are .h5 files in the directory\n",
    "if not h5_files:\n",
    "    raise FileNotFoundError(f\"No .h5 files found in the directory: {data_dir}\")\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_files, val_files = train_test_split(h5_files, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total samples: {len(h5_files)}\")\n",
    "print(f\"Training samples: {len(train_files)}\")\n",
    "print(f\"Validation samples: {len(val_files)}\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 8  # Adjust based on GPU memory\n",
    "\n",
    "image_dim = (128, 128)  # Adjust based on your image dimensions\n",
    "\n",
    "shuffle = True\n",
    "\n",
    "dataset = UNetDataset(train_files, data_dir, image_dim)\n",
    "    \n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_model(dataloader, model, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# # Loop through the data\n",
    "# for images, masks in dataloader:\n",
    "#     print(images.shape)  # Should be (batch_size, 4, 128, 128)\n",
    "#     print(masks.shape)   # Should be (batch_size, 1, 128, 128)\n",
    "\n",
    "# X_train, y_train = load_data(train_files, data_dir, image_dim=(128, 128))\n",
    "\n",
    "# X_val, y_val = load_data(val_files, data_dir, image_dim=(128, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "490f24c9-cad6-40f5-b998-e1b4ab6f990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_single_h5_file(filename):\n",
    "\n",
    "    # Load the image and mask from the HDF5 file\n",
    "    with h5py.File(filename, 'r') as h5_file:\n",
    "        image = h5_file[\"image\"][:]\n",
    "        mask = h5_file[\"mask\"][:]\n",
    "    \n",
    "    # --- Figure 1: Image with 4 channels ---\n",
    "    fig1, axs1 = plt.subplots(2, 2, figsize=(10, 10))\n",
    "    \n",
    "    Channel = [\"T1\", \"T1_weighted\", \"T2_weighted\", \"T2_FLAIR\"]\n",
    "    \n",
    "    # Plot each channel of the image in separate subplots\n",
    "    for i in range(4):\n",
    "        axs1[i//2, i%2].imshow(image[..., i])\n",
    "        axs1[i//2, i%2].set_title(f\"Channel: {Channel[i]}\")\n",
    "        axs1[i//2, i%2].axis('off')\n",
    "    \n",
    "    fig1.suptitle('Fig.1: Image with 4 Channels', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)  # Adjust title so it fits\n",
    "    plt.show()\n",
    "    \n",
    "    # --- Figure 2: Mask with 3 channels ---\n",
    "    fig2, axs2 = plt.subplots(1, 3, figsize=(10, 4))\n",
    "\n",
    "    Channel = [\"NCR/NET\", \"ED\",\"ET\"]\n",
    "    \n",
    "    # Plot each channel of the mask in separate subplots\n",
    "    for i in range(3):\n",
    "        axs2[i].imshow(mask[..., i])\n",
    "        axs2[i].set_title(f\"Channel:{Channel[i]}\")\n",
    "        axs2[i].axis('off')\n",
    "    \n",
    "    fig2.suptitle('Fig.2: Mask with 3 Channels', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.9)  # Adjust title so it fits\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eaf5428-59e0-4dbb-b357-24f04756283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming train_files and data_dir are defined\n",
    "# filename = os.path.join(data_dir, train_files[-1])\n",
    "\n",
    "# show_single_h5_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8766881-acc7-41fe-b9ce-62ab79040e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_slices(volume_index, slice_range, data_dir):\n",
    "    \"\"\"\n",
    "    Generates filenames for the MRI volume and slice range.\n",
    "\n",
    "    Parameters:\n",
    "    - volume_index: Index of the volume (e.g., 1 for 'volume_1', 2 for 'volume_2', etc.)\n",
    "    - slice_range: List of slice indices to load (e.g., [0, 1, 2, ..., 155])\n",
    "    - data_dir: Directory containing the HDF5 files.\n",
    "\n",
    "    Returns:\n",
    "    - filenames: List of filenames corresponding to the specified volume and slice range.\n",
    "    \"\"\"\n",
    "    image_list = []\n",
    "    mask_list=[]\n",
    "    for slice_idx in slice_range:\n",
    "        # Construct the filename\n",
    "        filename = os.path.join(data_dir, f\"volume_{volume_index}_slice_{slice_idx}.h5\")\n",
    "        \n",
    "        # Check if the file exists before adding to the list\n",
    "        if os.path.exists(filename):\n",
    "            with h5py.File(filename, 'r') as h5_file:\n",
    "                image_list.append(h5_file[\"image\"][:])\n",
    "                mask_list.append(h5_file[\"mask\"][:])\n",
    "        else:\n",
    "            print(f\"File {filename} does not exist.\")\n",
    "    \n",
    "    return np.transpose(np.array(image_list), (1, 2, 0, 3)), np.transpose(np.array(mask_list), (1, 2, 0, 3))\n",
    "\n",
    "# Example usage:\n",
    "volume_index = 100  # Volume number (e.g., 'volume_1')\n",
    "slice_range = range(0, 100)  # List of slices (e.g., slices 0, 1, 2)\n",
    "data_dir = \"BraTS2020_training_data/content/data/\"  # Path to your data directory\n",
    "# Get the filenames for the specified volume and slices\n",
    "image_list, mask_list = get_all_slices(volume_index, slice_range, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7379341d-46a1-433f-94d9-a34633a13fe5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2824b9fa2c0>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(mask_list[:,:,50,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
